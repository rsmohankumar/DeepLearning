{"cells":[{"cell_type":"markdown","id":"ce18792b-4c5d-40c8-acea-6cc6d0c59391","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"47eb0eae-8cc9-48ea-b5a4-6a263beb3760","metadata":{},"outputs":[],"source":["# <a id='toc1_'></a>[Convolutional Neural Network for Anime Image Classification](#toc0_)\n"]},{"cell_type":"markdown","id":"ab156d91-af37-4ea4-ba44-873bcfc3b250","metadata":{},"outputs":[],"source":["**Table of contents**<a id='toc0_'></a>    \n","- [Convolutional Neural Network for Anime Image Classification](#toc1_)    \n","  - [Objectives](#toc1_1_)    \n","  - [Setup](#toc1_2_)    \n","    - [Installing Required Libraries](#toc1_2_1_)    \n","    - [Importing Required Libraries](#toc1_2_2_)    \n","  - [Load the Data](#toc1_3_)    \n","    - [Visualize the Dataset](#toc1_3_1_)    \n","  - [Define Custom Dataset Class](#toc1_4_)    \n","  - [Split Dataset into Training and Validation Sets](#toc1_5_)    \n","  - [Define the CNN Model](#toc1_6_)    \n","  - [Define Loss Function and Optimizer](#toc1_7_)    \n","  - [Train the Model](#toc1_8_)    \n","  - [Visualize the Training and Test Loss](#toc1_9_)    \n","  - [Exercises](#toc1_10_)    \n","    - [Exercise 1 - Change activation function to `leaky_relu`](#toc1_10_1_)    \n","    - [Exercise 2 - Increase the number of epochs and observe the training and validation loss](#toc1_10_2_)    \n","    - [Exercise 3 - Use different character classes for training and validation](#toc1_10_3_)    \n","  - [Authors](#toc1_11_)    \n","  - [Contributors](#toc1_12_)    \n"]},{"cell_type":"markdown","id":"c6414441-39c0-48a7-8361-e27b022f57a2","metadata":{},"outputs":[],"source":["Estimated time needed: **30** minutes\n"]},{"cell_type":"markdown","id":"d9f21253-0158-4a49-b440-c01efe63f581","metadata":{},"outputs":[],"source":["This tutorial demonstrates how to build and train a **Convolutional Neural Network (CNN)** for image classification. The goal is to train a model that can classify images into the correct categories.\n"]},{"cell_type":"markdown","id":"c04cf924-3cb7-4261-9672-a82d0975d821","metadata":{},"outputs":[],"source":["## <a id='toc1_1_'></a>[Objectives](#toc0_)\n","\n","After completing this lab you will be able to:\n","\n"," - Apply **Convolutional Neural Network** to classify images.\n"]},{"cell_type":"markdown","id":"e1cbb045-fcc3-4c32-b9d6-c14ef7173db0","metadata":{},"outputs":[],"source":["----\n"]},{"cell_type":"markdown","id":"55e2e445-ddba-44db-9f7a-e5887506afbe","metadata":{},"outputs":[],"source":["## <a id='toc1_2_'></a>[Setup](#toc0_)\n","\n","For this lab, we will be using the following libraries:\n","\n","*   [`pandas`](https://pandas.pydata.org/) for managing the data.\n","*   [`numpy`](https://numpy.org/) for mathematical operations.\n","*   [`matplotlib`](https://matplotlib.org/) for additional plotting tools.\n","*   [`sklearn`](https://scikit-learn.org/stable/) for machine learning and machine-learning-pipeline related functions.\n","*   [`torch`](https://pytorch.org/) for building and training the deep neural network.\n","*   [`torchvision`](https://pytorch.org/vision/stable/index.html) for computer vision tasks.\n"]},{"cell_type":"markdown","id":"1545a9ba-603f-456f-b71f-6a0741f9bc3b","metadata":{},"outputs":[],"source":["### <a id='toc1_2_1_'></a>[Installing Required Libraries](#toc0_)\n","\n","The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n","\n","NOTE: It may take **a few minutes** to install the required libraries. Please be patient.\n"]},{"cell_type":"code","id":"4f1e5258-79cd-4f48-ba5f-d066ba171551","metadata":{},"outputs":[],"source":["!pip install pandas==2.2.2\n!pip install numpy==1.26.4\n!pip install matplotlib==3.8.0\n!pip install scikit-learn==1.5.0\n!pip install torch==2.3.1\n!pip install torchvision==0.18.1"]},{"cell_type":"markdown","id":"f1f9400c-a16a-4851-8387-ceca147c694b","metadata":{},"outputs":[],"source":["### <a id='toc1_2_2_'></a>[Importing Required Libraries](#toc0_)\n"]},{"cell_type":"code","id":"18a739cc-48b5-47ec-83d1-50fa871ab564","metadata":{},"outputs":[],"source":["import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport zipfile\nfrom PIL import Image"]},{"cell_type":"markdown","id":"968d3294-0be7-45ce-817f-98ad93294db0","metadata":{},"outputs":[],"source":["## <a id='toc1_3_'></a>[Load the Data](#toc0_)\n","\n","First, let's load our dataset and examine its structure.\n","\n","The data comes from the paper [AniWho: A Quick and Accurate Way to Classify Anime Character Faces in Images](https://arxiv.org/pdf/2208.11012v3). The dataset consists of 9,738 images across 130 character classes, with approximately 75 images per class, sourced from the Danbooru website—a platform developed by the Japanese animation-style cartoon community.\n","\n","For this tutorial, we will use a subset of the dataset. The zip file contains two subfolders: **Anastasia** and **Takao**, each with 50 images.\n","\n","Let's unzip the dataset.\n"]},{"cell_type":"code","id":"ad0f631b-979a-4b86-bea8-84f3b3a706a7","metadata":{},"outputs":[],"source":["import io\nimport requests\n\ndef load_images_from_zip(zip_file):\n    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n        images = {'anastasia': [], 'takao': []}\n        for file_name in zip_ref.namelist():\n            if file_name.startswith('anastasia') and file_name.endswith('.jpg'):\n                with zip_ref.open(file_name) as file:\n                    img = Image.open(file).convert('RGB')\n                    images['anastasia'].append(np.array(img))\n            elif file_name.startswith('takao') and file_name.endswith('.jpg'):\n                with zip_ref.open(file_name) as file:\n                    img = Image.open(file).convert('RGB')\n                    images['takao'].append(np.array(img))\n    return images\n\nzip_file_url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/xZQHOyN8ONT92kH-ASb4Pw/data.zip'\n\n# Download the ZIP file\nresponse = requests.get(zip_file_url)\nzip_file_bytes = io.BytesIO(response.content)\n\n# Load images from zip file\nimages = load_images_from_zip(zip_file_bytes)"]},{"cell_type":"markdown","id":"fe3fc1f5-e30b-42cb-b10d-a3cf3d61da12","metadata":{},"outputs":[],"source":["Let us check the number of images in each folder.\n"]},{"cell_type":"code","id":"ad7733fb-26d5-4953-82a3-fb78f4e13896","metadata":{},"outputs":[],"source":["print(\"Number of images of Anastasia:\", len(images['anastasia']))\nprint(\"Number of images of Takao:\", len(images['takao']))"]},{"cell_type":"markdown","id":"28fbc373-45a0-4635-bc54-d9998ce1006b","metadata":{},"outputs":[],"source":["### <a id='toc1_3_1_'></a>[Visualize the Dataset](#toc0_)\n","\n","Let's visualize images from the dataset. Since we have 50 images in each folder, we will display all the images in a grid.\n"]},{"cell_type":"code","id":"41f206a9-76b1-4af9-a942-8b29bcc60002","metadata":{},"outputs":[],"source":["def plot_images(images, title):\n    fig, axes = plt.subplots(5, 10, figsize=(10, 5))\n    fig.suptitle(title, fontsize=16)\n    axes = axes.flatten()\n    for img, ax in zip(images, axes):\n        ax.imshow(img)\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n# Plot images from 'anastasia'\nplot_images(images['anastasia'], 'Anastasia Images')\n\n# Plot images from 'takao'\nplot_images(images['takao'], 'Takao Images')"]},{"cell_type":"markdown","id":"c57b75b9-0a6a-49fc-a591-06777bc817b3","metadata":{},"outputs":[],"source":["## <a id='toc1_4_'></a>[Define Custom Dataset Class](#toc0_)\n","\n","We need to define a custom dataset class to load our images. This class will inherit from `torch.utils.data.Dataset`.\n"]},{"cell_type":"code","id":"da0e11ee-b636-49e3-8680-5ba13fb62f32","metadata":{},"outputs":[],"source":["class AnimeDataset(Dataset):\n    def __init__(self, images, transform=None, classes=None):\n        self.images = []\n        self.labels = []\n        self.transform = transform\n        self.classes = classes\n        \n        for label, class_name in enumerate(self.classes):\n            for img in images[class_name]:\n                self.images.append(img)\n                self.labels.append(label)\n    \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        image = Image.fromarray(self.images[idx])\n        label = self.labels[idx]\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label\n\n# Define transforms\ntransform = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\n# Load dataset\ndataset = AnimeDataset(images, transform=transform, classes=['anastasia', 'takao'])\n"]},{"cell_type":"markdown","id":"a6047240-461a-4b1d-b424-b9617b8d85d0","metadata":{},"outputs":[],"source":["## <a id='toc1_5_'></a>[Split Dataset into Training and Validation Sets](#toc0_)\n","\n","We will split the dataset into **training** and **validation** sets and create DataLoader for each.\n","\n","The data is split into **80%** training and **20%** validation sets.\n","\n","We then print the shapes of the training and test sets to verify that the data has been split correctly.\n"]},{"cell_type":"code","id":"f1d2bd79-08f2-4422-af4d-4b2c6ecbbf2a","metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\n# Set random seed for reproducibility\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# Generate a list of indices for the entire dataset\nindices = list(range(len(dataset)))\n\n# Split the indices into training and validation sets\ntrain_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=seed)\n\n# Create samplers for training and validation sets\ntrain_sampler = SubsetRandomSampler(train_indices)\nval_sampler = SubsetRandomSampler(val_indices)\n\n# Create DataLoader objects for training and validation sets\ntrain_loader = DataLoader(dataset, batch_size=8, sampler=train_sampler)\nval_loader = DataLoader(dataset, batch_size=20, sampler=val_sampler)\n\n# Print the sizes of the training and validation sets\nprint(\"Train size:\", len(train_indices))\nprint(\"Validation size:\", len(val_indices))"]},{"cell_type":"markdown","id":"d07c5d20-b8fc-4c0b-834e-a2784b6c6368","metadata":{},"outputs":[],"source":["## <a id='toc1_6_'></a>[Define the CNN Model](#toc0_)\n","\n","Now, let's define our CNN model. We will use `torch.nn` to build the model.\n"]},{"cell_type":"code","id":"ad8d83b7-5628-4240-86cf-412c50275c8b","metadata":{},"outputs":[],"source":["import torch.nn as nn\nimport torch.nn.functional as F\n\nclass AnimeCNN(nn.Module):\n    def __init__(self):\n        super(AnimeCNN, self).__init__()\n        # Add padding=1 to maintain the border\n        self.conv1 = nn.Conv2d(3, 32, 3, 1, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(64 * 16 * 16, 128)\n        self.fc2 = nn.Linear(128, 2)\n        \n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 64 * 16 * 16)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Instantiate the model\nmodel = AnimeCNN()"]},{"cell_type":"markdown","id":"6ed205a8-da8d-4d92-8b07-c8b64a7954bb","metadata":{},"outputs":[],"source":["Let us visualize the neural network architecture.\n"]},{"cell_type":"code","id":"0d228b71-2f4e-44fa-96c5-62dbd62a935b","metadata":{},"outputs":[],"source":["print(model)"]},{"cell_type":"markdown","id":"acb66112-21b6-4c09-8a8f-8019aa5fdca9","metadata":{},"outputs":[],"source":["Let us print the tensor shapes of each layer in the model.\n"]},{"cell_type":"code","id":"fd398cad-b85c-4381-b856-2b174526b8df","metadata":{},"outputs":[],"source":["input_tensor = torch.randn(1, 3, 64, 64)\n\ndef print_size(module, input, output):\n    print(f\"{module.__class__.__name__} output size: {output.size()}\")\n\n# Register hooks\nhooks = []\nfor layer in model.children():\n    hook = layer.register_forward_hook(print_size)\n    hooks.append(hook)\n\n# Inspect output sizes\nwith torch.no_grad():\n    output = model(input_tensor)\nprint(\"Final output size:\", output.size())\n\n# Remove hooks\nfor hook in hooks:\n    hook.remove()"]},{"cell_type":"markdown","id":"6c42ad87-e821-4aa6-ba52-a95aae5e7b6d","metadata":{},"outputs":[],"source":["## <a id='toc1_7_'></a>[Define Loss Function and Optimizer](#toc0_)\n","\n","We need to define the loss function and the optimizer. We will use CrossEntropyLoss and Adam optimizer.\n","\n","`CrossEntropyLoss` is used for multi-class classification problems.\n","\n","`Adam` is a popular optimization algorithm that is an extension of the stochastic gradient descent algorithm.\n"]},{"cell_type":"code","id":"2776267e-8fa3-44ee-b893-ee838a5392d9","metadata":{},"outputs":[],"source":["import torch.optim as optim\n\n# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"markdown","id":"eabca185-434e-4f49-abed-f7c240a9e343","metadata":{},"outputs":[],"source":["## <a id='toc1_8_'></a>[Train the Model](#toc0_)\n","\n","Now, let's train the model with our training data. We iterate over the training data for a specified number of epochs and update the weights of the neural network using backpropagation.\n","\n","During training, we calculate the loss at each epoch and print it to monitor the training progress. The loss should decrease over time as the model learns to classify the wines correctly.\n"]},{"cell_type":"code","id":"27c9d7b4-72b8-4c75-afd3-1dc60d0eb6a4","metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\nimport torch\n\n# Training loop\nnum_epochs = 5\ntrain_losses = []\nval_losses = []\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for i, data in enumerate(train_loader, 0):\n        inputs, labels = data\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    \n    train_loss = running_loss / len(train_loader)\n    train_losses.append(train_loss)\n    \n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for data in val_loader:\n            inputs, labels = data\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n    \n    val_loss = val_loss / len(val_loader)\n    val_losses.append(val_loss)\n    \n    print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n\nprint('Finished Training')\n\n"]},{"cell_type":"markdown","id":"8ac7790a-df25-4f6e-be7b-61160cf5d5b6","metadata":{},"outputs":[],"source":["## <a id='toc1_9_'></a>[Visualize the Training and Test Loss](#toc0_)\n","\n","We have successfully trained our model. Let's now visualize the training and test loss.\n","\n","Plotting the loss curves helps us understand the training dynamics of our model.\n"]},{"cell_type":"code","id":"bab24bd9-c1b0-47e2-973c-c9fb1f40ba9c","metadata":{},"outputs":[],"source":["# Plotting the training and validation loss\nplt.figure(figsize=(10, 5))\nplt.plot(train_losses, label='Training Loss')\nplt.plot(val_losses, label='Validation Loss', linestyle='--')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.title('Training and Validation Loss')\nplt.show()\n"]},{"cell_type":"markdown","id":"c28df602-2917-469d-8312-d322040a962e","metadata":{},"outputs":[],"source":["Finally, we evaluate the model on the validation data to see how well it performs on unseen data.\n","\n","We check the model's performance on individual images from the validation set. The model classifies most of the images correctly.\n"]},{"cell_type":"code","id":"48e7a14c-c96b-472a-8efb-84077314a3eb","metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\n# Function to display an image\ndef imshow(img, ax):\n    img = img / 2 + 0.5  # unnormalize\n    npimg = img.numpy()\n    ax.imshow(np.transpose(npimg, (1, 2, 0)))  # Transpose dimensions to match matplotlib's expected format\n    ax.axis('off')\n\n# Set model to evaluation mode\nmodel.eval()\n\ndata_iter = iter(val_loader)\nimages, labels = next(data_iter)\noutputs = model(images)\n_, predicted = torch.max(outputs, 1)\n\n# Define the grid size\nnum_images = len(images)\nnum_cols = 10\nnum_rows = 2\n\nfig, axs = plt.subplots(num_rows, num_cols * 2, figsize=(20, num_rows))\n\nfor idx in range(num_images):\n    row = idx // num_cols\n    col = (idx % num_cols) * 2\n    \n    # Plot the image\n    imshow(images[idx].cpu(), axs[row, col])\n    \n    # Display actual and predicted labels\n    axs[row, col + 1].text(0.5, 0.5, f\"Actual: {labels[idx].item()}\\nPredicted: {predicted[idx].item()}\",\n                           horizontalalignment='center', verticalalignment='center', fontsize=12)\n    axs[row, col + 1].axis('off')\n\n# Turn off any remaining empty subplots\nfor idx in range(num_images, num_rows * num_cols):\n    row = idx // num_cols\n    col = (idx % num_cols) * 2\n    axs[row, col].axis('off')\n    axs[row, col + 1].axis('off')\n\nplt.tight_layout()\nplt.show()"]},{"cell_type":"markdown","id":"4a44d5df-3547-4431-b49c-18ae70d7dc60","metadata":{},"outputs":[],"source":["We calculate the accuracy of the model on the validation set to measure its performance. \n","\n","The accuracy is the percentage of correctly classified samples in the validation set. A higher accuracy indicates better performance of the model. \n"]},{"cell_type":"code","id":"ad312f57-7e78-4dac-ba73-a2c5cafee017","metadata":{},"outputs":[],"source":["correct = 0\ntotal = 0\n\n# Compute overall accuracy\nwith torch.no_grad():\n    for data in val_loader:\n        images, labels = data\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        print(f'correct: {correct}, total: {total}')\n\nprint(f'Validation Accuracy: {100 * correct / total:.2f}%')"]},{"cell_type":"markdown","id":"cea33089-811c-4e45-a915-3c77900e7c85","metadata":{},"outputs":[],"source":["## <a id='toc1_10_'></a>[Exercises](#toc0_)\n"]},{"cell_type":"markdown","id":"2fa8858f-4546-49fc-8def-05e22b8b4db1","metadata":{},"outputs":[],"source":["### <a id='toc1_10_1_'></a>[Exercise 1 - Change activation function to `leaky_relu`](#toc0_)\n"]},{"cell_type":"code","id":"1d5c6c95-dae4-4ada-aa82-8669b5949e74","metadata":{},"outputs":[],"source":["class AnimeCNNModified(nn.Module):\n    def __init__(self):\n        super(AnimeCNNModified, self).__init__()\n        # Add padding=1 to maintain the border\n        self.conv1 = nn.Conv2d(3, 32, 3, 1, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(64 * 16 * 16, 128)\n        self.fc2 = nn.Linear(128, 2)\n        \n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))  # Change activation function here\n        x = self.pool(F.relu(self.conv2(x)))  # Change activation function here\n        x = x.view(-1, 64 * 16 * 16)\n        x = F.relu(self.fc1(x))  # Change activation function here\n        x = self.fc2(x)\n        return x"]},{"cell_type":"markdown","id":"41bd5db5-1450-4974-a347-7fe521c848c6","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","class AnimeCNNModified(nn.Module):\n","    def __init__(self):\n","        super(AnimeCNNModified, self).__init__()\n","        # Add padding=1 to maintain the border\n","        self.conv1 = nn.Conv2d(3, 32, 3, 1, padding=1)\n","        self.conv2 = nn.Conv2d(32, 64, 3, 1, padding=1)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.fc1 = nn.Linear(64 * 16 * 16, 128)\n","        self.fc2 = nn.Linear(128, 2)\n","        \n","    def forward(self, x):\n","        x = self.pool(F.leaky_relu(self.conv1(x)))  # Change activation function here\n","        x = self.pool(F.leaky_relu(self.conv2(x)))  # Change activation function here\n","        x = x.view(-1, 64 * 16 * 16)\n","        x = F.leaky_relu(self.fc1(x))  # Change activation function here\n","        x = self.fc2(x)\n","        return x\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"31972e62-aeca-4ed3-9b94-405741837b1e","metadata":{},"outputs":[],"source":["### <a id='toc1_10_2_'></a>[Exercise 2 - Increase the number of epochs and observe the training and validation loss](#toc0_)\n","\n","After increasing the number of epochs, you should observe that the training and validation loss decrease further. However, it may stop decreasing after a certain number of epochs.\n"]},{"cell_type":"code","id":"f30140c0-ddd4-4ac3-a1da-0cbf70a4df73","metadata":{},"outputs":[],"source":["# Instantiate the model\nmodel_new = AnimeCNN()\n\nimport torch.optim as optim\n\n# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_new.parameters(), lr=0.001)\n\nimport matplotlib.pyplot as plt\nimport torch\n\n# Training loop\n# TODO: Increase the number of epochs\nnum_epochs = 7 # Increase the number of epochs here\ntrain_losses = []\nval_losses = []\n\nfor epoch in range(num_epochs):\n    model_new.train()\n    running_loss = 0.0\n    for i, data in enumerate(train_loader, 0):\n        inputs, labels = data\n        optimizer.zero_grad()\n        outputs = model_new(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    \n    train_loss = running_loss / len(train_loader)\n    train_losses.append(train_loss)\n    \n    model_new.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for data in val_loader:\n            inputs, labels = data\n            outputs = model_new(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n    \n    val_loss = val_loss / len(val_loader)\n    val_losses.append(val_loss)\n    \n    print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n\nprint('Finished Training')\n\n# Plotting the training and validation loss\nplt.figure(figsize=(10, 5))\nplt.plot(train_losses, label='Training Loss')\nplt.plot(val_losses, label='Validation Loss', linestyle='--')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.title('Training and Validation Loss')\nplt.show()"]},{"cell_type":"markdown","id":"d157fc82-e471-404e-87b8-3cde04a24c18","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","num_epochs = 10 # Increase the number of epochs here\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"f7e3588d-37c2-460d-9ba2-70ad4ee91b2d","metadata":{},"outputs":[],"source":["### <a id='toc1_10_3_'></a>[Exercise 3 - Use different character classes for training and validation](#toc0_)\n","\n","You can find the code for reading the data below. Please follow the same steps to classify images from different character classes.\n","\n","This practice data contains images of two characters: **arcueid_brunestud** and **yukinoshita_yukino**.\n"]},{"cell_type":"code","id":"40c07489-54a8-4ee1-a1fd-61b812e8680e","metadata":{},"outputs":[],"source":["def load_images_from_zip(zip_file):\n    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n        images = {'arcueid_brunestud': [], 'yukinoshita_yukino': []}\n        for file_name in zip_ref.namelist():\n            if file_name.startswith('arcueid_brunestud') and file_name.endswith('.jpg'):\n                with zip_ref.open(file_name) as file:\n                    img = Image.open(file).convert('RGB')\n                    images['arcueid_brunestud'].append(np.array(img))\n            elif file_name.startswith('yukinoshita_yukino') and file_name.endswith('.jpg'):\n                with zip_ref.open(file_name) as file:\n                    img = Image.open(file).convert('RGB')\n                    images['yukinoshita_yukino'].append(np.array(img))\n    return images\n\nzip_file_url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/yNB99OssvDWOrNTHf2Yuxw/data-practice.zip'\n\n# Download the ZIP file\nresponse = requests.get(zip_file_url)\nzip_file_bytes = io.BytesIO(response.content)\n\n# Load images from zip file\nimages = load_images_from_zip(zip_file_bytes)\n\nprint(\"Number of images of arcueid_brunestud:\", len(images['arcueid_brunestud']))\nprint(\"Number of images of yukinoshita_yukino:\", len(images['yukinoshita_yukino']))\n\n# Plot images from 'arcueid_brunestud'\nplot_images(images['arcueid_brunestud'], 'arcueid_brunestud Images')\n\n# Plot images from 'yukinoshita_yukino'\nplot_images(images['yukinoshita_yukino'], 'yukinoshita_yukino Images')\n\n# TODO: Define a new AnimeDataset object for the new dataset.\n# TODO: Split the new dataset into training and validation sets.\n# TODO: Create DataLoader objects for the new training and validation sets.\n# TODO: Define a new CNN model and train it on the new dataset.\n# TODO: Evaluate the new model on the validation set."]},{"cell_type":"markdown","id":"72bf3972-1a11-4191-b3bb-405834b0df48","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for Solution (Entire code)</summary>\n","\n","```python\n","def load_images_from_zip(zip_file):\n","    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n","        images = {'arcueid_brunestud': [], 'yukinoshita_yukino': []}\n","        for file_name in zip_ref.namelist():\n","            if file_name.startswith('arcueid_brunestud') and file_name.endswith('.jpg'):\n","                with zip_ref.open(file_name) as file:\n","                    img = Image.open(file).convert('RGB')\n","                    images['arcueid_brunestud'].append(np.array(img))\n","            elif file_name.startswith('yukinoshita_yukino') and file_name.endswith('.jpg'):\n","                with zip_ref.open(file_name) as file:\n","                    img = Image.open(file).convert('RGB')\n","                    images['yukinoshita_yukino'].append(np.array(img))\n","    return images\n","\n","zip_file_url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/yNB99OssvDWOrNTHf2Yuxw/data-practice.zip'\n","\n","# Download the ZIP file\n","response = requests.get(zip_file_url)\n","zip_file_bytes = io.BytesIO(response.content)\n","\n","# Load images from zip file\n","images = load_images_from_zip(zip_file_bytes)\n","\n","print(\"Number of images of arcueid_brunestud:\", len(images['arcueid_brunestud']))\n","print(\"Number of images of yukinoshita_yukino:\", len(images['yukinoshita_yukino']))\n","\n","# Plot images from 'arcueid_brunestud'\n","plot_images(images['arcueid_brunestud'], 'arcueid_brunestud Images')\n","\n","# Plot images from 'yukinoshita_yukino'\n","plot_images(images['yukinoshita_yukino'], 'yukinoshita_yukino Images')\n","\n","# TODO: Define a new AnimeDataset object for the new dataset.\n","# TODO: Split the new dataset into training and validation sets.\n","# TODO: Create DataLoader objects for the new training and validation sets.\n","# TODO: Define a new CNN model and train it on the new dataset.\n","# TODO: Evaluate the new model on the validation set.\n","\n","# Load dataset\n","dataset = AnimeDataset(images, transform=transform, classes=['arcueid_brunestud', 'yukinoshita_yukino'])\n","\n","# Set random seed for reproducibility\n","seed = 42\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","\n","# Generate a list of indices for the entire dataset\n","indices = list(range(len(dataset)))\n","\n","# Split the indices into training and validation sets\n","train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=seed)\n","\n","# Create samplers for training and validation sets\n","train_sampler = SubsetRandomSampler(train_indices)\n","val_sampler = SubsetRandomSampler(val_indices)\n","\n","# Create DataLoader objects for training and validation sets\n","train_loader = DataLoader(dataset, batch_size=8, sampler=train_sampler)\n","val_loader = DataLoader(dataset, batch_size=20, sampler=val_sampler)\n","\n","# Print the sizes of the training and validation sets\n","print(\"Train size:\", len(train_indices))\n","print(\"Validation size:\", len(val_indices))\n","\n","# Instantiate the model\n","model_new_dataset = AnimeCNN()\n","\n","# Define the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model_new_dataset.parameters(), lr=0.001)\n","\n","# Training loop\n","num_epochs = 5\n","train_losses = []\n","val_losses = []\n","\n","for epoch in range(num_epochs):\n","    model_new_dataset.train()\n","    running_loss = 0.0\n","    for i, data in enumerate(train_loader, 0):\n","        inputs, labels = data\n","        optimizer.zero_grad()\n","        outputs = model_new_dataset(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","    \n","    train_loss = running_loss / len(train_loader)\n","    train_losses.append(train_loss)\n","    \n","    model_new_dataset.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for data in val_loader:\n","            inputs, labels = data\n","            outputs = model_new_dataset(inputs)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","    \n","    val_loss = val_loss / len(val_loader)\n","    val_losses.append(val_loss)\n","    \n","    print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n","\n","print('Finished Training')\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"e48debfa-556f-4f7f-93eb-f4de96512e4f","metadata":{},"outputs":[],"source":["## <a id='toc1_11_'></a>[Authors](#toc0_)\n"]},{"cell_type":"markdown","id":"28fca27a-cda1-4bb6-96d1-89add22c8759","metadata":{},"outputs":[],"source":["[Ricky Shi](https://www.linkedin.com/in/ricky-shi-ca/)\n"]},{"cell_type":"markdown","id":"7364abc2-78d1-455e-ab55-6656ed015324","metadata":{},"outputs":[],"source":["## <a id='toc1_12_'></a>[Contributors](#toc0_)\n"]},{"cell_type":"markdown","id":"c4a450f9-ee2a-4c2d-a1ba-6cff4678f003","metadata":{},"outputs":[],"source":["Copyright © 2024 IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"prev_pub_hash":"368de79595a0a52c4dc2b5a5da577fd6fe3ed51663ca94af5daf65099b0b5a67"},"nbformat":4,"nbformat_minor":4}